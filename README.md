# NER-on-MultiNERD
Evaluating DistilBERT on MultiNERD dataset

## Goal
We evaluate the performance of DistilBERT on the MultiNERT dataset. MultiNERD differs from other Named Entity Recognition datasets in that the classes are more fine-grained while also being autogenerated (and the text is also multi-lingual).
We compare two systems A: Training on all English examples in the dataset and B: Training on all English examples while only considering 6 classes, PER (Person), ORG (Organization), LOC (Location), DIS (Diseases), ANIM (Animal) and OTH (Other).

## Hypothesis
It is important to have a hypothesis before you start your experiments. While textual data is the same in both cases, the split of the classes is different. So, if a model is trying to decrease the error while training it will have to focus on predicting (and understanding) the 6 classes better in the case of system B. On the other hand, system A has to focus on 16 different classes to perform well. So our initial hypothesis is that system B will perform better.

## Running the code
The code is divided into two jupyter notebooks, one for each system. 
The notebooks can be run as they are in google colab without the need for any modifications (make sure you are connected to a GPU runtime).

## Methodology
During pre-processing we keep only the English language examples and in the case of system B change all classes other than the 5 discussed to OTH. We also tokenize the text in each sentence, this sometimes also includes splitting words into subword tokens (because of the way DistilBERT was trained initially). DistilBERT was chosen because it reaches the performance of BERT while also being smaller and faster to both train and predict. The problem with the tokenization we use is that there are now fewer labels than the number of tokens in our text. So we pad the labels for each example with OTH tokens whenever we encounter a subword that is not the beginning of a word. We also exclude them from the loss function (pytorch does this automatically when we set their label to a negative number). 

## Results
Each system is trained for two epochs and then validated on the test dataset. The results as as follows:

### System A

#### Training

| Epoch      | Training Loss | Validation Loss | Precision | Recall | F1 | Accuracy |
| ---------- | ------------- | --------------- | --------- | ------ | -- | -------- |
| 1 |	0.028200 | 0.053997 | 0.885613 | 0.894013 |	0.889793 | 0.982679 |
| 2	| 0.009300	| 0.069977	| 0.896311	| 0.905879	| 0.901070	| 0.983557 |

### System B

#### Training

| Epoch      | Training Loss | Validation Loss | Precision | Recall | F1 | Accuracy |
| ---------- | ------------- | --------------- | --------- | ------ | -- | -------- |
| 1 |	0.014100 | 0.031641 | 0.926654 | 0.941978 |	0.934253 | 0.990921 |
| 2	| 0.003500	| 0.040214	| 0.940534	| 0.942647	| 0.941589	| 0.991702 |

### On test set
| Metric | System A | System B |
| ------ | -------- | -------- |
| Accuracy | 0.986986 | 0.990729 |
| Precision | 0.913051 | 0.931925 |
| Recall | 0.928290 | 0.949174 |
| F1 | 0.920607 | 0.940470 |

As expected system B performs better than system A in all metrics. But the performance of system A is still impressive considering it has 3 times more classes to predict. This is probably down to the power and generalizability of DistilBERT. We can conduct further experiments like getting class-wise scores, and see how quickly each system trains to further understand the differences between the systems.


